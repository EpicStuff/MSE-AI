{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG using Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cohere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMuPDF\n",
    "\n",
    "import fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stuff like api keys\n",
    "from helpers import Dict\n",
    "from taml import taml\n",
    "# load stuff like api keys\n",
    "env = Dict(taml.load('env.taml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach-2 Tiered Knowledge space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain Meta Data from Zotero JSON file and Obtain Publication Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year(issued):\n",
    "    \"\"\"Extracts only the year from Zotero's issued date format.\"\"\"\n",
    "    if \"date-parts\" in issued and isinstance(issued[\"date-parts\"], list):\n",
    "        date_parts = issued[\"date-parts\"][0]  # Extract first date-parts entry\n",
    "        if len(date_parts) >= 1:\n",
    "            return str(date_parts[0])  # Return only the year\n",
    "    return \"Unknown Year\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 24 journal articles from Zotero.\n"
     ]
    }
   ],
   "source": [
    "# get meta data from json file of Zotero\n",
    "\n",
    "import json\n",
    "\n",
    "# Load Zotero JSON export file\n",
    "with open(\"Research Papers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    zotero_data = json.load(f)\n",
    "\n",
    "# Extract relevant metadata fields\n",
    "zotero_metadata = []\n",
    "for item in zotero_data:\n",
    "        metadata = {\n",
    "            \"title\": item.get(\"title\", \"Unknown Title\"),\n",
    "            \"abstract\": item.get(\"abstract\", \"Unknown Abstract\"),\n",
    "            \"DOI\": item.get(\"DOI\", \"Unknown Source\"),\n",
    "            \"year\": extract_year(item.get(\"issued\", {})),\n",
    "        }\n",
    "        zotero_metadata.append(metadata)    \n",
    "\n",
    "print(f\"Extracted {len(zotero_metadata)} journal articles from Zotero.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring profile with ReRank Relevance, Citations and year of publication\n",
    "\n",
    "Retrieved Contexts are ranked using scoring profile where Rerank Relevance is 70 % weightage. 20 % to number of citation and 10 percent for the year of publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Citation counts added to research papers.\n"
     ]
    }
   ],
   "source": [
    "# obtain number of citations for a research paper\n",
    "import re\n",
    "import requests\n",
    "\n",
    "def extract_arxiv_id(arxiv_number):\n",
    "    \"\"\"Extracts the numeric ArXiv ID from Zotero metadata.\"\"\"\n",
    "    match = re.search(r\"arXiv:(\\d+\\.\\d+)\", arxiv_number)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def get_citation_count(doi):\n",
    "    \"\"\"Fetches citation count using OpenCitations API if DOI is available.\"\"\"\n",
    "    if not doi:\n",
    "        return 0  # If no DOI, return 0\n",
    "    base_url = f\"https://opencitations.net/index/coci/api/v1/citations/{doi}\"\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code == 200:\n",
    "        return len(response.json())  # Number of citing papers\n",
    "    return 0\n",
    "\n",
    "def get_arxiv_citations(arxiv_number):\n",
    "    \"\"\"Fetches citation count for an ArXiv paper using Semantic Scholar API.\"\"\"\n",
    "    arxiv_id = extract_arxiv_id(arxiv_number)\n",
    "    if not arxiv_id:\n",
    "        return 0  # Return 0 if extraction fails\n",
    "\n",
    "    base_url = f\"https://api.semanticscholar.org/v1/paper/arXiv:{arxiv_id}\"\n",
    "    response = requests.get(base_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"citationCount\", 0)  # Extract citation count\n",
    "    return 0  # Return 0 if API fails\n",
    "\n",
    "\n",
    "def enrich_paper_with_citations(paper):\n",
    "    \"\"\"Adds citation count to a paper by checking DOI or ArXiv ID.\"\"\"\n",
    "    if 'number' in paper:\n",
    "        paper['arxiv_number'] = extract_arxiv_id(paper.get('number', ''))\n",
    "        paper['citations'] = get_arxiv_citations(paper['arxiv_number'])\n",
    "    elif \"DOI\" in paper:\n",
    "        paper[\"citations\"] = get_citation_count(paper[\"DOI\"])\n",
    "    else:\n",
    "        paper[\"citations\"] = 0  # Default if no identifier found\n",
    "    return paper\n",
    "\n",
    "# Apply citation retrieval to all papers in Zotero metadata\n",
    "for paper in zotero_metadata:\n",
    "    enrich_paper_with_citations(paper)\n",
    "\n",
    "\n",
    "\n",
    "print(\" Citation counts added to research papers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup of rerank function for retreival of research papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "# Rerank papers wrt user query \n",
    "def rerank_with_cohere(query, papers, top_n=20):\n",
    "    \"\"\"Ranks research papers using Cohere Rerank.\"\"\"\n",
    "    rerank_inputs = [f\"Title: {paper['title']} Abstract: {paper['abstract']}\" for paper in papers]\n",
    "    rerank_response = co.rerank(model=\"rerank-english-v2.0\", query=query, documents=rerank_inputs, top_n=top_n)\n",
    "    reranked_papers = [papers[result.index] for result in rerank_response.results]\n",
    "    \n",
    "    # Attach rerank scores to papers\n",
    "    for i, paper in enumerate(reranked_papers):\n",
    "        paper[\"rerank_score\"] = rerank_response.results[i].relevance_score\n",
    "    \n",
    "    return reranked_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton for research paper scoring profile \n",
    "import datetime\n",
    "\n",
    "# Initialize Cohere API\n",
    "co = cohere.Client(env.cohere_key_trial)\n",
    "\n",
    "def apply_scoring_profile(papers, weight_rerank=0.7, weight_citations=0.2, weight_year=0.1):\n",
    "    \"\"\"Applies a scoring profile to research papers.\"\"\"\n",
    "    current_year = datetime.datetime.now().year  # Get the current year\n",
    "    scored_papers = []\n",
    "    for paper in papers:\n",
    "        citations = int(paper.get(\"citations\", 0))\n",
    "        rerank_score = float(paper.get(\"rerank_score\", 0))\n",
    "        year = int(paper.get(\"year\", 2000))  # Default to 2000 if missing\n",
    "\n",
    "        # Normalize Year Score (latest year = higher score)\n",
    "        year_score = (year - 2000) / (current_year - 2000)  # Normalized between 0 and 1\n",
    "\n",
    "        # Normalize scores\n",
    "        citation_score = min(citations / 1000, 1)  # Normalize citations (cap at 1)\n",
    "\n",
    "        # Final Score\n",
    "        final_score = (rerank_score * weight_rerank) + (citation_score * weight_citations) + (year_score * weight_year)\n",
    "\n",
    "        # Store score\n",
    "        scored_papers.append((paper, final_score))\n",
    "\n",
    "    # Sort by highest score\n",
    "    scored_papers.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [paper for paper, score in scored_papers]\n",
    "\n",
    "def execute_ranking_pipeline(query, papers, top_n=3):\n",
    "    \"\"\"Executes full research paper ranking pipeline.\"\"\"\n",
    "    \n",
    "    # Step 1: Rerank using Cohere\n",
    "    reranked_papers = rerank_with_cohere(query, papers, top_n=5)\n",
    "    # print(reranked_papers)\n",
    "    \n",
    "    # # Step 2: Fetch citations (DOI & ArXiv support)\n",
    "    # for paper in reranked_papers:\n",
    "    #     if \"DOI\" in paper:\n",
    "    #         paper[\"citations\"] = get_citation_count(paper[\"DOI\"])\n",
    "    #     elif \"arxiv_id\" in paper:\n",
    "    #         paper[\"citations\"] = get_arxiv_citations(paper[\"arxiv_id\"])\n",
    "    #     else:\n",
    "    #         paper[\"citations\"] = 0  # Default if no identifier\n",
    "\n",
    "    # Step 3: Apply scoring profile (Relevance 0.7, Citations 0.2, Publicatio Year 0.1)\n",
    "    final_ranked_papers = apply_scoring_profile(reranked_papers)\n",
    "\n",
    "    # Return Top N Papers\n",
    "    return final_ranked_papers[:top_n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The future of self-driving laboratories from human in the loop interactive AI to gamification (Year: 2024, Citations: 0)\n",
      " Navigation maps of the material space for automated self driving labs of the future (Year: 2024, Citations: 0)\n",
      " Autonomous Chemical Experiments Challenges and Perspectives on Establishing a Self Driving Lab (Year: 2022, Citations: 37)\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "query = \"what is the future in self driving lab\"\n",
    "top_papers = execute_ranking_pipeline(query, zotero_metadata, top_n=3)\n",
    "\n",
    "# Print final ranked results\n",
    "for paper in top_papers:\n",
    "    print(f\" {paper['title']} (Year: {paper.get('year', 'Unknown')}, Citations: {paper['citations']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 2 Here we get the relevant context form the reranked research paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create function Chunks from PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chunk 1 ===\n",
      "Article ChemOS 2.0: An orchestration architecture for chemical self-driving laboratories ChemOS 2.0 is a comprehensive laboratory architecture for transforming the modern chemistry lab into one that accelerates the pace of chemical research. This new kind of laboratory, known as a self-driving lab (SDL), uses automated experimental tools, as well as computational experiment planners, to create fully automated workﬂows that require minimal human intervention. ChemOS 2.0 presents a modular and versatile approach to building one’s own SDL, including real-life implementations of this framework. Malcolm Sim, Mohammad Ghazi Vakili, Felix Strieth-Kalthoff, ..., Santiago Miret, Sergio Pablo-Garcı´a, Ala´n Aspuru-Guzik spgarcica@gmail.com (S.P.-G.) alan@aspuru.com (A.A.-G.) Highlights A modular strategy for building a self-driving lab for chemical research Demonstrative workﬂows based on real-world research in materials discovery High- and low-level implementation of laboratory hardware/software Automated experiment planning and execution as well as automated data collection Sim et al., Matter 7, 2959–2977 September 4, 2024 ª 2024 Elsevier Inc. All rights reserved. https://doi.org/10.1016/j.matt.2024.04.022 ll Article ChemOS 2.0: An orchestration architecture for chemical self-driving laboratories Malcolm Sim,1,2 Mohammad Ghazi Vakili,1,2 Felix Strieth-Kalthoff,1,2 Han Hao,1,2 Riley J. Hickman,1,2,3 Santiago Miret,4 Sergio Pablo-Garcı´a,1,2,3,9,* and Ala´n Aspuru-Guzik1,2,3,5,6,7,8,* \n",
      "\n",
      "=== Chunk 2 ===\n",
      "(A.A.-G.) https://doi.org/10.1016/j.matt.2024.04.022 ll 2960 Matter 7, 2959–2977, September 4, 2024 Article the full orchestration of a complex materials discovery workﬂow toward novel gain materials for organic solid-state lasing devices. RESULTS Software management Meticulous control over the laboratory’s software ecosystem is imperative to enhance experimental reproducibility, increase transparency, and mitigate produc- tion failures. As such, achieving complete transparency necessitates stringent man- agement of the software state within a laboratory with dependency conﬂicts due to incompatible software versions posing a well-recognized challenge in this regard.36 To address these concerns, the core design of ChemOS 2.0 incorporates an orches- tration fog device that runs the necessary software layers for laboratory operations while keeping each laboratory modular for streamlined integration. To ensure repro- ducibility, robustness, and seamless deployment, the fog orchestration platform is equipped with NixOS, a declarative package-manager-based (Nix) operating sys- tem known for its ability to provide precise control over system state and software A B C D E F Figure 1. Features and capabilities of ChemOS 2.0 (A) Web graphical interface to ease the user interaction. (B) Package to ensure full software reproducibility. (C) Bayesian optimizer platform for experimental planning. (D) DFT workﬂow manager connected to our high-performance computer cluster to orchestrate ab ini\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract full text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    \n",
    "    for page in doc:\n",
    "        full_text += page.get_text(\"text\") + \"\\n\"\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "def chunk_text(text, chunk_size=1500, overlap=150):\n",
    "    \"\"\"Chunk text into fixed-length segments with overlap.\"\"\"\n",
    "    words = text.split()  # Tokenizing by words (for simplicity)\n",
    "    chunks = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap  # Shift start position with overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "pdf_path = r\"C:\\Users\\Chayan\\OneDrive - University of Toronto\\Desktop\\Winter 25\\M.Eng Project\\Cohere Approach\\pdfs\\1-s2.0-S2590238524001954-main.pdf\"\n",
    "full_text = extract_text_from_pdf(pdf_path)\n",
    "chunks = chunk_text(full_text, chunk_size=1500, overlap=150)\n",
    "\n",
    "# Print the first 2 chunks for preview\n",
    "for i, chunk in enumerate(chunks[:2]):\n",
    "    print(f\"=== Chunk {i+1} ===\\n{chunk[:1500]}\\n\")  # Show first 500 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder= r\"C:\\Users\\Chayan\\OneDrive - University of Toronto\\Desktop\\Winter 25\\M.Eng Project\\Cohere Approach\\pdfs\\Final Papers\"\n",
    "\n",
    "for paper in top_papers:\n",
    "    pdf_path = os.path.join(pdf_folder, f\"{paper['title']}.pdf\")  # Locate PDF by title\n",
    "\n",
    "    if os.path.exists(pdf_path):\n",
    "        full_text = extract_text_from_pdf(pdf_path)  # Extract full text\n",
    "        chunks = chunk_text(full_text, chunk_size=1500, overlap=150)  # Chunk the text\n",
    "\n",
    "        # Store chunks in paper dictionary\n",
    "        paper[\"chunks\"] = chunks\n",
    "    else:\n",
    "        print(f\"PDF not found: {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rerank_chunks(query, chunks, top_n=3):\n",
    "    \"\"\"Ranks research papers using Cohere Rerank.\"\"\"\n",
    "    rerank_response = co.rerank(model=\"rerank-english-v2.0\", query=query, documents=chunks, top_n=top_n)\n",
    "    reranked_papers = [chunks[result.index] for result in rerank_response.results]\n",
    "    \n",
    "    # Attach rerank scores to papers\n",
    "    for i, paper in enumerate(reranked_papers):\n",
    "        paper[\"rerank_score\"] = rerank_response.results[i].relevance_score\n",
    "    \n",
    "    return reranked_papers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def execute_full_pipeline(query, zotero_metadata, pdf_folder, top_n_papers=3, chunk_size=1500, overlap=150, top_n_chunks=5):\n",
    "    \"\"\"\n",
    "    Execute Phase 1 (Retrieve top papers) and Phase 2 (Extract, chunk, and rerank text).\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query for retrieving relevant papers.\n",
    "        zotero_metadata (dict): Metadata from Zotero containing paper titles and other details.\n",
    "        pdf_folder (str): The folder containing the research PDFs.\n",
    "        top_n_papers (int): Number of top-ranked papers to retrieve.\n",
    "        chunk_size (int): Size of text chunks.\n",
    "        overlap (int): Overlap between chunks.\n",
    "        top_n_chunks (int): Number of top-ranked chunks to return.\n",
    "\n",
    "    Returns:\n",
    "        list: Top-ranked chunks after reranking.\n",
    "    \"\"\"\n",
    "    \n",
    "    # **Phase 1: Retrieve Top Papers**\n",
    "    top_papers = execute_ranking_pipeline(query, zotero_metadata, top_n=top_n_papers)\n",
    "\n",
    "    # **Phase 2: Extract and Chunk Text**\n",
    "    for paper in top_papers:\n",
    "        pdf_path = os.path.join(pdf_folder, f\"{paper['title']}.pdf\")  # Locate PDF by title\n",
    "        \n",
    "        if os.path.exists(pdf_path):\n",
    "            full_text = extract_text_from_pdf(pdf_path)  # Extract text from PDF\n",
    "            chunks = chunk_text(full_text, chunk_size=chunk_size, overlap=overlap)  # Chunking\n",
    "            \n",
    "            # Store chunks in paper dictionary\n",
    "            paper[\"chunks\"] = chunks\n",
    "        else:\n",
    "            print(f\"PDF not found: {pdf_path}\")\n",
    "            paper[\"chunks\"] = []  # Assign empty list if file is missing\n",
    "\n",
    "    # **Prepare Chunks for Reranking**\n",
    "    all_chunks = [{\"title\": paper[\"title\"], \"text\": chunk} for paper in top_papers for chunk in paper[\"chunks\"]]\n",
    "\n",
    "    # **Rerank Chunks**\n",
    "    ranked_chunks = rerank_chunks(query, all_chunks, top_n=top_n_chunks)\n",
    "\n",
    "    return ranked_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the challenges of self-driving labs?\"\n",
    "pdf_folder= r\"C:\\Users\\Chayan\\OneDrive - University of Toronto\\Desktop\\Winter 25\\M.Eng Project\\Cohere Approach\\pdfs\\Final Papers\"\n",
    "\n",
    "# Execute full pipeline (Phase 1 + Phase 2)\n",
    "ranked_chunks = execute_full_pipeline(query, zotero_metadata, pdf_folder)\n",
    "\n",
    "# Print the top-ranked chunks\n",
    "for i, chunk in enumerate(ranked_chunks):\n",
    "    print(f\"=== Chunk {i+1} from {chunk['title']} ===\\n{chunk['text'][:2500]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is  self-driving labs?\"\n",
    "pdf_folder= r\"C:\\Users\\Chayan\\OneDrive - University of Toronto\\Desktop\\Winter 25\\M.Eng Project\\Cohere Approach\\pdfs\\Final Papers\"\n",
    "\n",
    "# Execute Retrieval full pipeline (Phase 1 + Phase 2)\n",
    "ranked_chunks = execute_full_pipeline(query, zotero_metadata, pdf_folder)\n",
    "\n",
    "# Print the top-ranked chunks\n",
    "for i, chunk in enumerate(ranked_chunks):\n",
    "    print(f\"=== Chunk {i+1} from {chunk['title']} ===\\n{chunk['text'][:2500]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Responses from the retrieved context chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere  # Cohere API client\n",
    "\n",
    "# Initialize Cohere client (replace with your API key)\n",
    "co = cohere.Client(api_key=env.cohere_key)\n",
    "\n",
    "def response_to_chunks(query):\n",
    "    \"\"\"\n",
    "    Generates an LLM response based on retrieved and reranked research chunks.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        ranked_chunks (list): List of top-ranked text chunks.\n",
    "        model (str): Cohere model name for chat generation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Generated response text, citations)\n",
    "    \"\"\"\n",
    "    model=\"command-r-plus-08-2024\"\n",
    "    # Execute Retrieval full pipeline (Phase 1 + Phase 2)\n",
    "    ranked_chunks = execute_full_pipeline(query, zotero_metadata, pdf_folder)\n",
    "    \n",
    "    # **Fix: Ensure `data` is a String (Not a Dictionary)**\n",
    "    documents = [{\"data\": f\"{chunk['title']}: {chunk['text']}\"} for chunk in ranked_chunks]\n",
    "\n",
    "    # **System Prompt for Context Awareness**\n",
    "    system_prompt = (\n",
    "        \"You are an AI research assistant specializing in self-driving labs and automation. \"\n",
    "        \"Use the provided research excerpts to answer queries accurately. \"\n",
    "        \"Avoid speculation—if an answer is not found in the documents, state it explicitly.\"\n",
    "    )\n",
    "\n",
    "    # **Format the Query**\n",
    "    user_message = (\n",
    "        f\"Format your response as follows:\\n\"\n",
    "        \"**Query:** {query}\\n\"\n",
    "        \"- **Summary:** (Concise response)\\n\"\n",
    "        \"- **Key points:** (List the most important points)\\n\"\n",
    "        \"- **Sources:** (List titles of the relevant research excerpts)\"\n",
    "        \n",
    "    )\n",
    "\n",
    "    # **Call Cohere's `chat()` API Correctly**\n",
    "    response = co.chat(\n",
    "        model=model,\n",
    "        message=user_message,  # Fix: Using `message` instead of `messages`\n",
    "        documents=documents,  # Pass the context as `documents`\n",
    "        preamble=system_prompt  # Fix: Use `preamble` for system instructions\n",
    "    )\n",
    "\n",
    "    return response.text, response.citations  # Extract response text & citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "document id=doc_0 is too long and may provide bad results, please chunk your documents to 300 words or less, document id=doc_1 is too long and may provide bad results, please chunk your documents to 300 words or less, document id=doc_2 is too long and may provide bad results, please chunk your documents to 300 words or less, document id=doc_3 is too long and may provide bad results, please chunk your documents to 300 words or less, document id=doc_4 is too long and may provide bad results, please chunk your documents to 300 words or less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL RESPONSE FROM LLM ===\n",
      "**Query:** What are the challenges of building a self-driving lab?\n",
      "- **Summary:** Building a self-driving lab is challenging due to the need to replace human cognitive processes and motor functions with automation. This includes replicating human actions that require fine motor skills and hand-eye coordination, as well as handling unexpected results and optimizing algorithms with constraints.\n",
      "- **Key points:**\n",
      "- Replacing human cognitive processes with ML algorithms is difficult due to the need to automate instruments designed for human use, handle unexpected results, and optimize algorithms with constraints.\n",
      "- Replacing or replicating motor functions is challenging, especially for actions requiring fine motor skills and hand-eye coordination, such as troubleshooting electrochemical experiments.\n",
      "- Adapting experimental procedures designed for human experimenters is not straightforward, and there may be more efficient ways to achieve the same goal in an automated fashion.\n",
      "- Software control and integration is a practical challenge, as few instrument manufacturers design their products with self-driving laboratories in mind.\n",
      "- Handling heterogeneous systems, such as dispensing solids or performing extractions, is a significant challenge.\n",
      "- Automation is expensive, limiting its accessibility to groups and countries with sufficient budgets.\n",
      "- **Sources:** Autonomous Chemical Experiments Challenges and Perspectives on Establishing a Self Driving Lab, Performance metrics to unleash the power of self-driving labs in chemistry and materials science\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the challenges of self-driving labs?\"\n",
    "\n",
    "# Execute full pipeline (Phase 1 + Phase 2 + Phase 3)\n",
    "final_response, citation = response_to_chunks(query)\n",
    "\n",
    "# Print the response from the model\n",
    "print(\"=== FINAL RESPONSE FROM LLM ===\")\n",
    "print(final_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "document id=doc_0 is too long and may provide bad results, please chunk your documents to 300 words or less, document id=doc_1 is too long and may provide bad results, please chunk your documents to 300 words or less, document id=doc_2 is too long and may provide bad results, please chunk your documents to 300 words or less, document id=doc_3 is too long and may provide bad results, please chunk your documents to 300 words or less, document id=doc_4 is too long and may provide bad results, please chunk your documents to 300 words or less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL RESPONSE FROM LLM ===\n",
      "**Query:** What are the challenges of building a self-driving lab?\n",
      "- **Summary:** The challenges of building a self-driving lab can be divided into two categories: cognitive processes and motor function.\n",
      "- **Key points:**\n",
      "  - Cognitive processes:\n",
      "    - Replacing human cognitive processes with ML algorithms in the \"real world\" of chemistry can be difficult, as unexpected or hard-to-predict results may occur.\n",
      "    - Automating instruments designed for human use can be challenging.\n",
      "    - Most optimization algorithms assume the absence of holes and known constraints, but real data often has unforeseen holes and unknown constraints.\n",
      "    - Automated identification of unknown compounds is challenging, and unexpected or unknown side products can form.\n",
      "    - The susceptibility to ionization and manner in which molecules ionize is not always predictable a priori.\n",
      "    - Few manufacturers develop their software to consider self-driving laboratories, requiring significant time investment to write custom code for instrument adaptation.\n",
      "  - Motor function:\n",
      "    - Replacing or replicating certain actions that are easy for humans due to their fine motor skills and hand-eye coordination can be challenging.\n",
      "    - Troubleshooting electrochemical experiments, for example, is more suited to human actions because it requires dexterity and coordination between visual inputs and motor response.\n",
      "- **Sources:** Autonomous Chemical Experiments Challenges and Perspectives on Establishing a Self Driving Lab, Research Trend Analysis in the Field of Self Driving Lab Using Network Analysis and Topic Modeling, A self driving laboratory optimizes a scalable process for making functional coatings\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the research trends in the field of self driving labs\"\n",
    "\n",
    "# Execute full pipeline (Phase 1 + Phase 2 + Phase 3)\n",
    "final_response, citation = response_to_chunks(query)\n",
    "\n",
    "# Print the response from the model\n",
    "print(\"=== FINAL RESPONSE FROM LLM ===\")\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the future  of self driving labs\"\n",
    "\n",
    "# Execute full pipeline (Phase 1 + Phase 2 + Phase 3)\n",
    "final_response, citation = response_to_chunks(query)\n",
    "\n",
    "# Print the response from the model\n",
    "print(\"=== FINAL RESPONSE FROM LLM ===\")\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
