{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from epicstuff import Dict\n",
    "from taml import taml\n",
    "\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stuff like api keys\n",
    "env = Dict(taml.load('env.taml'))\n",
    "env.ai_api_ver = '2024-02-01'\n",
    "env.ai_model = 'text-embedding-ada-002'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pdf to txt\n",
    "def pdf_to_text(pdf_path):\n",
    "\tfrom PyPDF2 import PdfReader\n",
    "\treader = PdfReader(pdf_path)\n",
    "\ttext = ''\n",
    "\tfor page in reader.pages:\n",
    "\t\ttext += page.extract_text()\n",
    "\treturn text\n",
    "\n",
    "def convert_pdfs_in_folder(pdf_folder, output_folder):\n",
    "\t# Ensure the output folder exists\n",
    "\tos.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\t# Loop through all files in the directory\n",
    "\tfor filename in os.listdir(pdf_folder):\n",
    "\t\tif filename.endswith(\".pdf\"):\n",
    "\t\t\tpdf_path = os.path.join(pdf_folder, filename)\n",
    "\t\t\ttext = pdf_to_text(pdf_path)\n",
    "\n",
    "\t\t\tif text:  # If text was extracted\n",
    "\t\t\t\t# Create a text file with the same name as the PDF\n",
    "\t\t\t\toutput_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "\t\t\t\toutput_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "\t\t\t\t# Save the extracted text to the file\n",
    "\t\t\t\twith open(output_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "\t\t\t\t\ttext_file.write(text)\n",
    "\t\t\t\tprint(f\"Converted {filename} to {output_filename}\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(f\"No text found in {filename}\")\n",
    "\n",
    "\n",
    "pdf_folder = \"pdfs\"  # Folder containing PDFs\n",
    "output_folder = \"Final/texts\"  # Folder to save text files\n",
    "\n",
    "convert_pdfs_in_folder(pdf_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azure search object from azuresearch.ipynb\n",
    "\n",
    "from azure.search.documents.indexes.models import FreshnessScoringFunction, FreshnessScoringParameters, ScoringProfile, SearchableField, SearchField, SearchFieldDataType, \\\n",
    "\tSimpleField, TextWeights\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "\tazure_deployment=env.ai_model,\n",
    "\topenai_api_version=env.ai_api_ver,\n",
    "\tazure_endpoint=env.ai_endpoint,\n",
    "\tapi_key=env.ai_api,\n",
    ")\n",
    "embedding_function = embeddings.embed_query\n",
    "\n",
    "fields = [\n",
    "\tSimpleField(\n",
    "\t\tname='id',\n",
    "\t\ttype=SearchFieldDataType.String,\n",
    "\t\tkey=True,\n",
    "\t\tfilterable=True,\n",
    "\t),\n",
    "\tSearchableField(\n",
    "\t\tname='content',\n",
    "\t\ttype=SearchFieldDataType.String,\n",
    "\t\tsearchable=True,\n",
    "\t),\n",
    "\tSearchField(\n",
    "\t\tname='content_vector',\n",
    "\t\ttype=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "\t\tsearchable=True,\n",
    "\t\tvector_search_dimensions=len(embedding_function('Text')),\n",
    "\t\tvector_search_profile_name='myHnswProfile',\n",
    "\t),\n",
    "\tSearchableField(\n",
    "\t\tname='metadata',\n",
    "\t\ttype=SearchFieldDataType.String,\n",
    "\t\tsearchable=True,\n",
    "\t),\n",
    "\t# Additional field to store the title\n",
    "\tSearchableField(\n",
    "\t\tname='title',\n",
    "\t\ttype=SearchFieldDataType.String,\n",
    "\t\tsearchable=True,\n",
    "\t),\n",
    "\t# Additional field for filtering on document source\n",
    "\tSimpleField(\n",
    "\t\tname='source',\n",
    "\t\ttype=SearchFieldDataType.String,\n",
    "\t\tfilterable=True,\n",
    "\t),\n",
    "\t# Additional data field for last doc update\n",
    "\tSimpleField(\n",
    "\t\tname='last_update',\n",
    "\t\ttype=SearchFieldDataType.DateTimeOffset,\n",
    "\t\tsearchable=True,\n",
    "\t\tfilterable=True,\n",
    "\t),\n",
    "]\n",
    "# Adding a custom scoring profile with a freshness function\n",
    "sc_name = 'scoring_profile'\n",
    "sc = ScoringProfile(\n",
    "\tname=sc_name,\n",
    "\ttext_weights=TextWeights(weights={'title': 5}),\n",
    "\tfunction_aggregation='sum',\n",
    "\tfunctions=[\n",
    "\t\tFreshnessScoringFunction(\n",
    "\t\t\tfield_name='last_update',\n",
    "\t\t\tboost=100,\n",
    "\t\t\tparameters=FreshnessScoringParameters(boosting_duration='P2D'),\n",
    "\t\t\tinterpolation='linear',\n",
    "\t\t)\n",
    "\t],\n",
    ")\n",
    "\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "\tazure_search_endpoint=env.search_endpoint,\n",
    "\tazure_search_key=env.search_api,\n",
    "\tindex_name='langchain-vector-demo-custom-scoring-profile',\n",
    "\tembedding_function=embeddings.embed_query,\n",
    "\tfields=fields,\n",
    "\tscoring_profiles=[sc],\n",
    "\tdefault_scoring_profile=sc_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "doc_list = []\n",
    "# load each text document into azure search\n",
    "for file in glob(f'{output_folder}/*.txt'):\n",
    "\n",
    "\tloader = TextLoader(file, encoding=\"utf-8\")\n",
    "\n",
    "\tdocuments = loader.load()\n",
    "\ttext_splitter = CharacterTextSplitter(chunk_size=700, chunk_overlap=50, separator='.')\n",
    "\tdocs = text_splitter.split_documents(documents)\n",
    "\tdoc_list.append(docs)\n",
    "\tprint(docs)\n",
    "\tprint(len(docs))   #how many chunks are there in each pdf\n",
    "\tvector_store.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in doc_list[0]:   #see how the doc is splitted\n",
    "    print(doc.page_content)\n",
    "    print('-----------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Cohere\n",
    "from langchain_core.prompts  import PromptTemplate\n",
    "#from langchain_cohere import Cohere\n",
    "\n",
    "cohere_llm = Cohere(model=\"command\",\n",
    "                    temperature=0.1,\n",
    "                    cohere_api_key = \"xEVt3zKBQ7aPsG05ZM7EwlCdMnhZLZwfR2s7T0tu\")\n",
    "\n",
    "prompt_template = \"\"\"Answer the question as precise as possible using the provided context. If the answer is\n",
    "                not contained in the context, say \"answer not available in context\" \\n\\n\n",
    "                Context: \\n {context}?\\n\n",
    "                Question: \\n {question} \\n\n",
    "                Answer:\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=prompt_template)\n",
    "\n",
    "# Formatting the docs for the RAG chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find embending \"group\" with greatest similarity to query\n",
    "#vector_store.similarity_search(query=\"How can the degree of autonomy of SDL be broken down into different levels?\", k=3, search_type=\"similarity\")[0].page_content\n",
    "vector_store.similarity_search(query=\"How long does it take to build an SDL?\", k=3, search_type=\"similarity\")[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Chain\n",
    "def generate_answer(question):\n",
    "    llm = cohere_llm\n",
    "    docs = vector_store.similarity_search(query=question, k=3, search_type=\"similarity\")[0]\n",
    "    context = docs.page_content\n",
    "\n",
    "    #allow prompt truncation\n",
    "    return llm(prompt.format(context = context, question = question), truncate = 'START')\n",
    "\n",
    "# Generate answer\n",
    "#generate_answer(\"What challenges are associated with optimizing SDL performance across different studies?\")\n",
    "generate_answer('How can the degree of autonomy of SDL be broken down into different levels?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
