{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install langchain-community langchain-openai langchain-core\n",
    "pip install azure-search-documents azure-core\n",
    "pip install azure-identity\n",
    "pip install cohere\n",
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from helpers import Dict\n",
    "from taml import taml\n",
    "\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stuff like api keys\n",
    "env = Dict(taml.load('env.taml'))\n",
    "env.ai_api_ver = '2024-02-01'\n",
    "env.ai_model = 'text-embedding-ada-002'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pdf to txt\n",
    "def pdf_to_text(pdf_path):\n",
    "\tfrom PyPDF2 import PdfReader\n",
    "\treader = PdfReader(pdf_path)\n",
    "\ttext = ''\n",
    "\tfor page in reader.pages:\n",
    "\t\ttext += page.extract_text()\n",
    "\treturn text\n",
    "\n",
    "def convert_pdfs_in_folder(pdf_folder, output_folder):\n",
    "\t# Ensure the output folder exists\n",
    "\tos.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\t# Loop through all files in the directory\n",
    "\tfor filename in os.listdir(pdf_folder):\n",
    "\t\tif filename.endswith(\".pdf\"):\n",
    "\t\t\tpdf_path = os.path.join(pdf_folder, filename)\n",
    "\t\t\ttext = pdf_to_text(pdf_path)\n",
    "\n",
    "\t\t\tif text:  # If text was extracted\n",
    "\t\t\t\t# Create a text file with the same name as the PDF\n",
    "\t\t\t\toutput_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "\t\t\t\toutput_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "\t\t\t\t# Save the extracted text to the file\n",
    "\t\t\t\twith open(output_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "\t\t\t\t\ttext_file.write(text)\n",
    "\t\t\t\tprint(f\"Converted {filename} to {output_filename}\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(f\"No text found in {filename}\")\n",
    "\n",
    "\n",
    "pdf_folder = \"pdfs/2023-2024 research articles\"  # Folder containing PDFs\n",
    "output_folder = \"Final/texts\"  # Folder to save text files\n",
    "\n",
    "convert_pdfs_in_folder(pdf_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azure search object from azuresearch.ipynb\n",
    "\n",
    "from azure.search.documents.indexes.models import FreshnessScoringFunction, FreshnessScoringParameters, ScoringProfile, SearchableField, SearchField, SearchFieldDataType, \\\n",
    "\tSimpleField, TextWeights\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "\tazure_deployment=env.ai_model,\n",
    "\topenai_api_version=env.ai_api_ver,\n",
    "\tazure_endpoint=env.ai_endpoint,\n",
    "\tapi_key=env.ai_api,\n",
    ")\n",
    "embedding_function = embeddings.embed_query\n",
    "\n",
    "fields = [\n",
    "\tSimpleField(\n",
    "\t\tname='id',\n",
    "\t\ttype=SearchFieldDataType.String,\n",
    "\t\tkey=True,\n",
    "\t\tfilterable=True,\n",
    "\t),\n",
    "\tSearchableField(\n",
    "\t\tname='content',\n",
    "\t\ttype=SearchFieldDataType.String,\n",
    "\t\tsearchable=True,\n",
    "\t),\n",
    "\tSearchField(\n",
    "\t\tname='content_vector',\n",
    "\t\ttype=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "\t\tsearchable=True,\n",
    "\t\tvector_search_dimensions=len(embedding_function('Text')),\n",
    "\t\tvector_search_profile_name='myHnswProfile',\n",
    "\t),\n",
    "\tSearchableField(\n",
    "\t\tname='metadata',\n",
    "\t\ttype=SearchFieldDataType.String,\n",
    "\t\tsearchable=True,\n",
    "\t),\n",
    "\t# Additional field to store the title\n",
    "\tSearchableField(\n",
    "\t\tname='title',\n",
    "\t\ttype=SearchFieldDataType.String,\n",
    "\t\tsearchable=True,\n",
    "\t),\n",
    "\t# Additional field for filtering on document source\n",
    "\tSimpleField(\n",
    "\t\tname='source',\n",
    "\t\ttype=SearchFieldDataType.String,\n",
    "\t\tfilterable=True,\n",
    "\t),\n",
    "\t# Additional data field for last doc update\n",
    "\tSimpleField(\n",
    "\t\tname='last_update',\n",
    "\t\ttype=SearchFieldDataType.DateTimeOffset,\n",
    "\t\tsearchable=True,\n",
    "\t\tfilterable=True,\n",
    "\t),\n",
    "]\n",
    "# Adding a custom scoring profile with a freshness function\n",
    "sc_name = 'scoring_profile'\n",
    "sc = ScoringProfile(\n",
    "\tname=sc_name,\n",
    "\ttext_weights=TextWeights(weights={'title': 5}),\n",
    "\tfunction_aggregation='sum',\n",
    "\tfunctions=[\n",
    "\t\tFreshnessScoringFunction(\n",
    "\t\t\tfield_name='last_update',\n",
    "\t\t\tboost=100,\n",
    "\t\t\tparameters=FreshnessScoringParameters(boosting_duration='P2D'),\n",
    "\t\t\tinterpolation='linear',\n",
    "\t\t)\n",
    "\t],\n",
    ")\n",
    "\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "\tazure_search_endpoint=env.search_endpoint,\n",
    "\tazure_search_key=env.search_api,\n",
    "\tindex_name='langchain-vector-demo-custom-scoring-profile',\n",
    "\tembedding_function=embeddings.embed_query,\n",
    "\tfields=fields,\n",
    "\tscoring_profiles=[sc],\n",
    "\tdefault_scoring_profile=sc_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "doc_list = []\n",
    "# load each text document into azure search\n",
    "for file in glob(f'{output_folder}/*.txt'):\n",
    "\n",
    "\tloader = TextLoader(file, encoding=\"utf-8\")\n",
    "\n",
    "\tdocuments = loader.load()\n",
    "\ttext_splitter = CharacterTextSplitter(chunk_size=700, chunk_overlap=50, separator='.')\n",
    "\tdocs = text_splitter.split_documents(documents)\n",
    "\tdoc_list.append(docs)\n",
    "\tprint(docs)\n",
    "\tprint(len(docs))  # how many chunks are there in each pdf\n",
    "\t# vector_store.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in doc_list[14]:  # see how the doc is splitted\n",
    "    print(doc.page_content)\n",
    "    print('-----------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Cohere\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_cohere import Cohere\n",
    "\n",
    "cohere_llm = Cohere(model=\"command\",\n",
    "                    temperature=0.1,\n",
    "                    cohere_api_key=env.cohere_key)\n",
    "\n",
    "prompt_template = \"\"\"Answer the question with the provided context.\" \\n\\n\n",
    "                Context: \\n {context}?\\n\n",
    "                Question: \\n {question} \\n\n",
    "                Answer:\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=prompt_template)\n",
    "\n",
    "# Formatting the docs for the RAG chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find embending \"group\" with greatest similarity to query\n",
    "# vector_store.similarity_search(query=\"How can the degree of autonomy of SDL be broken down into different levels?\", k=3, search_type=\"similarity\")[0].page_content\n",
    "vector_store.similarity_search(query=\"What are the main challenges in the translation of protocols of self-driving labs?\", k=3, search_type=\"similarity\")[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = 'Alhpa flow is a self driving lab system. What learning method does the system use?'\n",
    "multi_choice1 = 'A. Active learning, B. Unsupervised learning, C. Reinforcement learning, D. English learning'  # correct answer is C\n",
    "question2 = 'sciclops is used in one of the self driving lab systems. What is sciclops?'\n",
    "multi_choice2 = 'A. a robotic arm, B. a microchip, C. a software, D. a microscope'  # correct answer is A\n",
    "question3 = 'BMLP enables rapid optimisation of metabolic models and offers a realistic approach to a self-driving lab for microbial engineering. What does BMLP stand for?'\n",
    "multi_choice3 = 'A. Bayesian Metabolic Learning Platform, B. Bayesian Metabolic Learning Process, C. Boolean Matrix Logic Programming , D. Bayesian Matrix Learning Protocol'  # correct answer is C\n",
    "question4 = 'In the Adam system, what is recommended form King et al.?'\n",
    "multi_choice4 = 'A. routine supervision, B. more advanced hardware, C. complete autonomous lab, D. manual operation'  # correct answer is A\n",
    "question5 = 'BRAD is a state-of-the-art chatbot and agentic system that integrates a suite of tools to handle bioinformatics tasks. What does BRAD stand for?'\n",
    "multi_choice5 = 'A. Bioinformatics Research and Development, B.  Bioinformatics Retrieval Augmented Data, ssC. Bayesian Rapid Automated Discovery, D. Bioinformatics Rapid Automated Discovery'  # correct answer is B\n",
    "question6 = 'What scale is the chemical space of all possible molecules is often estimated at?'\n",
    "multi_choice6 = 'A. 10^40, B. 10^50, C. 10^60, D. 10^70'  # correct answer is C\n",
    "question7 = 'In the paper, GPC quantifies alterations in the hydrodynamic radius associated with molecular weight. What does GPC stand for?'\n",
    "multi_choice7 = 'A. Gel Permeation Chromatography, B. Gel Permeation Coefficient, C. Gas Permeation Constant, D. Gas Permeation Chromatography'  # correct answer is D\n",
    "question8 = 'What are the main challenges in the translation of protocols of self-driving labs?'\n",
    "multi_choice8 = 'A. data, hardware, and software; B. language, syntax, and semantics; C. data, autonomy, and execution; D. syntax, semantics, and execution'  # correct answer is D\n",
    "question9 = 'In the study that uses social networking services to operate scanning probe microscopy measurement systems, when user message is judged as not executable, how does the system respond?'\n",
    "multi_choice9 = 'A. the system does not respond; B. the system still tries to execute the command; C. the system prints none and give the reason; D. the system will pause for manual operation'  # correct answer is C\n",
    "question10 = 'The concept of SDLs has its roots in the broader field of laboratory automation, began in:'\n",
    "multi_choice10 = 'A. mid-19th century, B. early-20th century, C. mid-20th century, D. in 21st century'  # correct answer is C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Chain\n",
    "def generate_answer(question, multi_choice):  # question and the multiple choice are separated, and only the question is used to search for the context.\n",
    "    question_template = '\\n Only one answer is correct. Only print out the one answer and the one letter (A,B,C,D) that represents it, and nothing else.'\n",
    "    llm = cohere_llm\n",
    "    docs = format_docs(vector_store.similarity_search(query=question, k=4, search_type=\"similarity\"))\n",
    "    # docs = vector_store.similarity_search(query=question, k=3, search_type=\"similarity\")[1]\n",
    "    context = docs.replace('\\x00', '')  # remove mail characters as processing of the text\n",
    "\n",
    "    # print(prompt.format(context = context, question = question + multi_choice))\n",
    "    question = question + multi_choice + question_template\n",
    "\n",
    "    # allow prompt truncation\n",
    "    return llm(prompt.format(context=context, question=question), truncate='START')\n",
    "\n",
    "\n",
    "def without_rag(question):\n",
    "    question_template = 'Only one answer is correct, only print out the one answer and the one letter (A,B,C,D) that represents it.'\n",
    "    question = question + question_template\n",
    "    llm = cohere_llm\n",
    "    return llm(question, truncate='START')\n",
    "\n",
    "\n",
    "n = 1\n",
    "\n",
    "for question, multi_choice in zip([question1, question2, question3, question4, question5, question6, question7, question8, question9, question10],\n",
    "                                  [multi_choice1, multi_choice2, multi_choice3, multi_choice4, multi_choice5, multi_choice6, multi_choice7, multi_choice8, multi_choice9, multi_choice10]):\n",
    "    print(n)\n",
    "    print(generate_answer(question, multi_choice))\n",
    "    print('------------------')\n",
    "    print(without_rag(question + multi_choice))\n",
    "    print('==================')\n",
    "    n += 1\n",
    "\n",
    "\n",
    "# question = question8\n",
    "# multi_choice = multi_choice8\n",
    "\n",
    "# print(generate_answer(question, multi_choice))\n",
    "# print('------------------')\n",
    "# print(without_rag(question+multi_choice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM with and without RAG are tested with the 10 questions, repeated for 5 times. LLM-RAG scores 9/10 for 4 times and 8/10 for 1 time. LLM without RAG scores 3/10 for 5 times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate QA pairs\n",
    "\n",
    "def generate_qa_pairs(context, cohere_llm, temperature=0.5, max_tokens=500):\n",
    "    prompt = f\"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information.\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your question)\n",
    "Answer: (your answer)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\n",
    "\"\"\"\n",
    "    try:\n",
    "        # Generate the response\n",
    "        response = cohere_llm.generate(\n",
    "            prompts=[prompt],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            stop_sequences=[\"Output:::\"]\n",
    "        )\n",
    "        \n",
    "        # Extract the text from the first generation of the first list\n",
    "        generated_text = response.generations[0][0].text.strip()\n",
    "        \n",
    "        return generated_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating QA pair: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "qa_pairs = []  # To store the generated QA pairs\n",
    "\n",
    "# Process all documents and chunks with a progress bar\n",
    "for doc in tqdm(doc_list, desc=\"Processing Documents\"):  # Iterate over each document in the list\n",
    "    for chunk in tqdm(doc, desc=\"Processing Chunks\", leave=False):  # Iterate over each chunk in the document\n",
    "        try:\n",
    "            # Extract the page content (context) from the chunk\n",
    "            context = chunk.page_content\n",
    "            \n",
    "            # Generate QA pairs\n",
    "            qa_text = generate_qa_pairs(context, cohere_llm)\n",
    "            if qa_text:\n",
    "                # Extract the question and answer\n",
    "                question = qa_text.split(\"Factoid question: \")[-1].split(\"Answer: \")[0].strip()\n",
    "                answer = qa_text.split(\"Answer: \")[-1].strip()\n",
    "                \n",
    "                # Append to the QA pairs list\n",
    "                qa_pairs.append({\"context\": context, \"question\": question, \"answer\": answer})\n",
    "            else:\n",
    "                print(\"No QA pair generated for chunk:\", context)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating QA pair: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code ended with the error due to limit of free api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame for easier inspection\n",
    "qa_df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# Save the DataFrame as a CSV for analysis\n",
    "qa_df.to_csv(\"qa_pairs.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the Judge-LLM model\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Initialize the client with the Hugging Face API token\n",
    "client = InferenceClient(api_key=env.hf_api_key)\n",
    "\n",
    "# Function to evaluate a question-answer pair using Llama model\n",
    "def judgeLLM(question, answer, context):\n",
    "    # Format the critique prompt\n",
    "    critique_prompt = f\"\"\"\n",
    "    Evaluate the following question, answer, and context based on these criteria:\n",
    "    1. Groundedness: Can the question be answered from the given context? (Add 1 point if yes)\n",
    "    2. Stand-alone: Can the question be understood without the context? (Add 1 point if yes)\n",
    "    3. Faithfulness: Does the answer match the context? (Add 1 point if yes)\n",
    "    4. Answer Relevance: Does the answer directly address the question? (Add 1 point if yes)\n",
    "\n",
    "    Your response should strictly follow this format:\n",
    "    \n",
    "    Evaluation: (Provide a brief rationale for your rating)\n",
    "    Total rating: (A number between 0 and 4)\n",
    "\n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    Context: {context}\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the messages for Llama API call\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": critique_prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Make the API call to the Llama model (change the model if necessary)\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",  # Or another accessible model\n",
    "        messages=messages,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    \n",
    "    # Extract the result\n",
    "    evaluation = completion.choices[0].message['content']\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# Example usage\n",
    "question = \"Where is the Eiffel Tower located?\"\n",
    "answer = \"Paris\"\n",
    "context = \"The Eiffel Tower is located in Paris.\"\n",
    "\n",
    "# Call the judgeLLM function and print the evaluation\n",
    "evaluation_result = judgeLLM(question, answer, context)\n",
    "print(evaluation_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "qa_eval_task = pd.read_csv('qa_pairs.csv')\n",
    "\n",
    "# Function to evaluate the loaded DataFrame\n",
    "def evaluate_loaded_df(df):\n",
    "    evaluations = []\n",
    "    \n",
    "    # Wrap the loop with tqdm for progress visualization\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Evaluating QA pairs\"):\n",
    "        question = row[\"question\"]\n",
    "        answer = row[\"answer\"]\n",
    "        context = row[\"context\"]\n",
    "        \n",
    "        # Call the judgeLLM function for each question-answer pair\n",
    "        evaluation_result = judgeLLM(question, answer, context)\n",
    "        \n",
    "        # Store the result\n",
    "        evaluations.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"context\": context,\n",
    "            \"evaluation\": evaluation_result\n",
    "        })\n",
    "    \n",
    "    # Convert to a DataFrame for better visualization\n",
    "    result_df = pd.DataFrame(evaluations)\n",
    "    return result_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the loaded DataFrame\n",
    "QA_evaluation_result_df = evaluate_loaded_df(qa_eval_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the evaluation results to a new CSV file\n",
    "QA_evaluation_result_df.to_csv('qa_eval_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
